{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b49bb2e-065e-4a53-9b13-c80ee076489c",
   "metadata": {},
   "source": [
    "# Ground Truth Cleaning #\n",
    "\n",
    "### Pipeline processed most recently: Mar 07, 2024 ###\n",
    "\n",
    "**_All data collected from TalkBank's AphasiaBank under permission. Data are not public._**\n",
    "\n",
    "Output data are located in `'../../data/'`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a15edab4-52c0-4fda-a5b8-54eb636f3d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-07\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pylangacq as pla\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "current_date = datetime.now().strftime('%Y-%m-%d')\n",
    "print(current_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ba4bdf-9ffd-4387-92e6-8e7fc3bd193f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Clean `raw_transcription` and create `clean_groundtruth`. Create `aphasia/control_concat_fix.csv` file. ###\n",
    "\n",
    "## 3-1. Clean `raw_transcription` and create `clean` and `clean_original`. ##\n",
    "\n",
    "`clean_original` is the programmatically cleaned groundtruth without manual checking below. Use `clean` in the later final output files.\n",
    "`clean_transcription_error` generates version of groundtruth with the intended \"target\" speech, with speech errors corrected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fa7ad18-b9a4-4664-b0c1-891b51398992",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_transcription(transcription):\n",
    "        \n",
    "    transcription = transcription.replace('&-', 'FILLER')\n",
    "    transcription = transcription.replace('&+', 'FRAGMENT')\n",
    "    \n",
    "    transcription = transcription.replace('[<]', '').replace('[.]', '').replace('[>]', '')\n",
    "    transcription = transcription.replace('[>1]', '').replace('[>2]', '').replace('[>3]', '').replace('[<1]', '').replace('[<2]', '').replace('[<3]', '')\n",
    "    transcription = transcription.replace('+<', '').replace('+,', '').replace('+..?', '').replace('+..', '').replace('+/?', '').replace('[?]', '')\n",
    "    transcription = transcription.replace('(.)', '').replace('(..)', '').replace('(...)', '')\n",
    "    transcription = transcription.replace('<', '').replace('>', '')\n",
    "    transcription = transcription.replace('(', '').replace(')', '')\n",
    "    transcription = transcription.replace('‡', '').replace('[/]', '').replace('[//]', '').replace('[///]', '').replace('[/?]', '').replace('[/-]', '')\n",
    "    \n",
    "    # Remove [+ ~]\n",
    "    transcription = transcription.replace('[+ gram]', '').replace('[+ gram', '').replace('[+ gra', '').replace('[+ gr', '').replace('[+ g', '')\n",
    "    transcription = transcription.replace('[+ exc]', '').replace('[+ exc', '').replace('[+ ex', '')\n",
    "    transcription = transcription.replace('[+ esc]', '').replace('[+ esc', '').replace('[+ es]', '').replace('[+ es', '').replace('[+ e', '')\n",
    "    transcription = transcription.replace('[+ jar]', '').replace('[+ per]', '').replace('[+ jar', '').replace('[+ ja', '').replace('[+ j', '')\n",
    "    transcription = transcription.replace('[+ circ]', '').replace('[+ cir]', '').replace('[+ cir', '').replace('[+ ci', '').replace('[+ c', '')\n",
    "    transcription = transcription.replace('[+ ', '').replace('[+', '')\n",
    "    \n",
    "    # Replace xxx\n",
    "    transcription = transcription.replace('xxx', 'UNK')\n",
    "    \n",
    "    # Error words\n",
    "    # on [: and] [* p:w] -> on\n",
    "    # transcription = re.sub(r\"\\s([a-zA-Z'_]+)\\s\\[\\:\\s([a-zA-Z'_\\s]+)\\]\\s\\[\\*\\s([a-zA-Z'_]*\\:[a-z-]*)\\]\", r' \\1', transcription)\n",
    "    transcription = re.sub(r\"(?:^|\\s)([a-zA-Z'_-]+)\\s\\[\\:\\s([a-zA-Z'\\s_-]+)\\]\\s\\[\\*\\s*([*:a-zA-Z\\d\\s'+-=]*)\\]\", r' \\1', transcription)\n",
    "\n",
    "    # ain't [: are not] (without error code) -> ain't\n",
    "    transcription = re.sub(r\"(?:^|\\s)([a-zA-Z'_-]+)\\s\\[\\:\\s([a-zA-Z'\\s_-]+)\\](?!\\s\\[)\", r' \\1', transcription)\n",
    "    \n",
    "    # honli@u [: only] [* p:n] -> honli\n",
    "    transcription = re.sub(r\"(?:^|\\s)([a-zA-Z'_-]+)@u\\s\\[\\:\\s([a-zA-Z'_@\\s]*)\\]\\s\\[\\*\\s*([*:a-zA-Z\\d\\s'+-=]*)\\]\", r' \\1', transcription)\n",
    "         \n",
    "    # kotəgəl@u [: comfortable] [* n:k] -> UNK\n",
    "    # hɑspəlɪd@u [: hospital] [* n:k-ret] -> UNK\n",
    "    # ðæɾɪ@u [: x@n] [* n:uk] -> UNK\n",
    "    # mɔ@u [: x@n] [* n:uk-rep] -> UNK\n",
    "    # fɪŋks@u [: sphinx] [*] -> UNK\n",
    "    transcription = re.sub(r\"(\\S*@u)\\s\\[\\:\\s([a-zA-Z'_@\\s]*)\\]\\s\\[\\*\\s*([*:a-zA-Z\\d\\s'+-=]*)\\]\", r'UNK', transcription)\n",
    "\n",
    "    # ʌt [: up] [* p:n] -> UNK\n",
    "    # iʔi [: x@n] [* n:uk] -> UNK\n",
    "    transcription = re.sub(r\"([a-zA-Z'_]*[^a-zA-Z'_\\s]+[a-zA-Z'_]*[^a-zA-Z'_\\s]*[a-zA-Z'_]*[^a-zA-Z'_\\s]*[a-zA-Z'_]*[^a-zA-Z'_\\s]*[a-zA-Z'_]*[^a-zA-Z'_\\s]*[a-zA-Z'_]*[^a-zA-Z'_\\s]*[a-zA-Z'_]*[^a-zA-Z'_\\s]*)\\s\\[\\:\\s([a-zA-Z'_@\\s]+)\\]\\s\\[\\*\\s*([*:a-zA-Z\\s'+-=]*)\\]\", r'UNK', transcription)\n",
    "    transcription = re.sub(r\"\\S*\\s\\[\\:\\s([a-zA-Z'_@$\\s]+)\\]\\s\\[\\*\\s*([*:a-zA-Z\\s'+-=]*)\\]\", r'UNK', transcription)\n",
    "                 \n",
    "    # sɪnrɛlə@u [: Cinderella] (without error code) -> UNK\n",
    "    # transcription = re.sub(r\"(\\S*@u)\\s\\[\\:\\s([a-zA-Z'_@\\s]*)\\](?!\\s\\[\\*)\", r'UNK', transcription)\n",
    "    transcription = re.sub(r\"([a-zA-Z'_]*[^a-zA-Z'_\\s]+[a-zA-Z'_]*[^a-zA-Z'_\\s]*[a-zA-Z'_]*[^a-zA-Z'_\\s]*[a-zA-Z'_]*[^a-zA-Z'_\\s]*[a-zA-Z'_]*[^a-zA-Z'_\\s]*[a-zA-Z'_]*[^a-zA-Z'_\\s]*[a-zA-Z'_]*[^a-zA-Z'_\\s]*)\\s\\[\\:\\s([a-zA-Z'_@\\s]+)\\]\", r'UNK', transcription)\n",
    "    \n",
    "    # walked [: s:uk-ret] (without replacement, error code typo) -> walked\n",
    "    transcription = re.sub(r\"(?:^|\\s)([a-zA-Z'_-]+)\\s\\[\\:\\s([a-zA-Z':\\s-]+)\\](?!\\s\\[\\*)\", r' \\1', transcription)\n",
    "    \n",
    "    # remaining error codes\n",
    "    transcription = re.sub(r'\\[\\*\\s[^\\]]+\\]', '', transcription)\n",
    "    transcription = transcription.replace('[*]', '')\n",
    "    \n",
    "    # untackled cases\n",
    "    transcription = re.sub(r\"(\\S*@u)\\s\\[\\:\\s(\\S*)\\]\", 'UNK', transcription)\n",
    "    transcription = re.sub(r\"([a-zA-Z'_]*(?!@)[^a-zA-Z'_\\s]+[a-zA-Z'_]*)\\s\\[\\:\\s([a-zA-Z'_@\\s]+)\\]\", 'UNK', transcription)\n",
    "    \n",
    "    \n",
    "    # start of the string\n",
    "    # transcription = re.sub(r\"([a-zA-Z']+)\\s\\[:\\s([a-zA-Z']+)\\]\", r'\\1', transcription)\n",
    "                           \n",
    "    # Error codes\n",
    "    transcription = re.sub(r'\\$\\S+', '', transcription)  \n",
    "    # transcription = re.sub(r'@l', '', transcription)\n",
    "    # change letter to upper class\n",
    "    transcription = re.sub(r'\\b([a-zA-Z])@l\\b', lambda m: m.group(1).upper(), transcription)\n",
    "    transcription = re.sub(r'@o', '', transcription)\n",
    "    transcription = re.sub(r'@b', '', transcription)\n",
    "    transcription = re.sub(r'@q', '', transcription)\n",
    "    transcription = re.sub(r'@k', '', transcription)\n",
    "    transcription = re.sub(r'@i', '', transcription)\n",
    "    # transcription = re.sub(r'@n', '', transcription) \n",
    "    transcription = re.sub(r'@si', '', transcription)\n",
    "    \n",
    "    transcription = re.sub(r'([a-zA-Z]+)@n', r'\\1', transcription)\n",
    "    transcription = re.sub(r'\\S+@u', r'UNK', transcription)\n",
    "\n",
    "    # INV\n",
    "    transcription = re.sub(r'&\\*INV\\S+', '', transcription)\n",
    "    \n",
    "    # Remove words with &-, &+, &=, [=! ], [= ], [!], [% ]\n",
    "    # need to include & because typo\n",
    "    #transcription = re.sub(r'&[+-]?.*?\\s', '', transcription)\n",
    "    transcription = re.sub(r'&\\S*', '', transcription)\n",
    "    transcription = re.sub(r'&[^ ]*', '', transcription)\n",
    "    # transcription = re.sub(r'\\[=!\\s[a-zA-Z]*\\]', '', transcription)\n",
    "    transcription = transcription.replace('[!]', '')\n",
    "    transcription = re.sub(r'\\[=[^\\]]+\\]', '', transcription)\n",
    "    transcription = transcription.replace('[=! laughin', '')\n",
    "    transcription = re.sub(r\"\\[%\\s(.*)\\]\", '', transcription)\n",
    "    \n",
    "    # Remove unnecessary chars\n",
    "    transcription = transcription.replace('+', '').replace('\"', '').replace('...', '').replace('//', '').replace('/', '').replace('^', '').replace('„', '')\n",
    "    \n",
    "    # Replace _ with space\n",
    "    transcription = transcription.replace('_', ' ')\n",
    "\n",
    "    # Remove punctuation\n",
    "    transcription = transcription.replace('.', ' ')\n",
    "    transcription = transcription.replace('?', ' ')\n",
    "    transcription = transcription.replace('!', ' ')\n",
    "    transcription = transcription.replace('”', '').replace('“', '')  \n",
    "     \n",
    "    # Remove :\n",
    "    transcription = transcription.replace(':', '')\n",
    "    \n",
    "    # Replace - with whitespace\n",
    "    transcription = transcription.replace('-', ' ')\n",
    "    \n",
    "    # Remove words containing '0' - there's error code with '0\n",
    "    transcription = ' '.join(word for word in transcription.split() if '0' not in word)\n",
    "    \n",
    "    # Standardize whitespace\n",
    "    transcription = re.sub(r'\\s+', ' ', transcription)\n",
    "    \n",
    "    # Unknown - I suspect linebreak\n",
    "    transcription = transcription.replace('\u0015', '')\n",
    "\n",
    "    # transcription = transcription.replace('FILLER', '&-')\n",
    "    # transcription = transcription.replace('FRAGMENT', '&+')\n",
    "    \n",
    "    return transcription.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dbaf18f3-f847-4ed4-b4c9-08b991401ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning function for speech errors - in progress\n",
    "def clean_transcription_error(transcription):\n",
    "        \n",
    "    transcription = transcription.replace('&-', 'FILLER')\n",
    "    transcription = transcription.replace('&+', 'FRAGMENT')\n",
    "    \n",
    "    transcription = transcription.replace('[<]', '').replace('[.]', '').replace('[>]', '')\n",
    "    transcription = transcription.replace('[>1]', '').replace('[>2]', '').replace('[>3]', '').replace('[<1]', '').replace('[<2]', '').replace('[<3]', '')\n",
    "    transcription = transcription.replace('+<', '').replace('+,', '').replace('+..?', '').replace('+..', '').replace('+/?', '').replace('[?]', '')\n",
    "    transcription = transcription.replace('(.)', '').replace('(..)', '').replace('(...)', '')\n",
    "    transcription = transcription.replace('<', '').replace('>', '')\n",
    "    transcription = transcription.replace('(', '').replace(')', '')\n",
    "    transcription = transcription.replace('‡', '').replace('[/]', '').replace('[//]', '').replace('[///]', '').replace('[/?]', '').replace('[/-]', '')\n",
    "    \n",
    "    # Remove [+ ~]\n",
    "    transcription = transcription.replace('[+ gram]', '').replace('[+ gram', '').replace('[+ gra', '').replace('[+ gr', '').replace('[+ g', '')\n",
    "    transcription = transcription.replace('[+ exc]', '').replace('[+ exc', '').replace('[+ ex', '')\n",
    "    transcription = transcription.replace('[+ esc]', '').replace('[+ esc', '').replace('[+ es]', '').replace('[+ es', '').replace('[+ e', '')\n",
    "    transcription = transcription.replace('[+ jar]', '').replace('[+ per]', '').replace('[+ jar', '').replace('[+ ja', '').replace('[+ j', '')\n",
    "    transcription = transcription.replace('[+ circ]', '').replace('[+ cir]', '').replace('[+ cir', '').replace('[+ ci', '').replace('[+ c', '')\n",
    "    transcription = transcription.replace('[+ ', '').replace('[+', '')\n",
    "    \n",
    "    # Replace xxx\n",
    "    transcription = transcription.replace('xxx', 'UNK')\n",
    "    \n",
    "    # keep semantic errors\n",
    "    transcription = re.sub(r\"(?:^|\\s)([a-zA-Z'_-]+)(?:\\s\\[\\:\\s([a-zA-Z'\\s_-]+)\\])?\\s\\[\\*\\s*s:\\s*([*:a-zA-Z\\d\\s'+-=]*)\\]\", r' \\1', transcription)    \n",
    "\n",
    "    # keep morphological errors\n",
    "    transcription = re.sub(r\"(?:^|\\s)([a-zA-Z'_-]+)(?:\\s\\[\\:\\s([a-zA-Z'\\s_-]+)\\])?\\s\\[\\*\\s*m:\\s*([*:a-zA-Z\\d\\s'+-=]*)\\]\", r' \\1', transcription)    \n",
    "\n",
    "    # Error words\n",
    "    # on [: and] [* p:w] -> on\n",
    "    # transcription = re.sub(r\"\\s([a-zA-Z'_]+)\\s\\[\\:\\s([a-zA-Z'_\\s]+)\\]\\s\\[\\*\\s([a-zA-Z'_]*\\:[a-z-]*)\\]\", r' \\1', transcription)\n",
    "    transcription = re.sub(r\"(?:^|\\s)([a-zA-Z'_-]+)\\s\\[\\:\\s([a-zA-Z'\\s_-]+)\\]\\s\\[\\*\\s*([*:a-zA-Z\\d\\s'+-=]*)\\]\", r' \\1', transcription)    \n",
    "\n",
    "    # ain't [: are not] (without error code) -> ain't\n",
    "    transcription = re.sub(r\"(?:^|\\s)([a-zA-Z'_-]+)\\s\\[\\:\\s([a-zA-Z'\\s_-]+)\\](?!\\s\\[)\", r' \\1', transcription)\n",
    "    \n",
    "    # honli@u [: only] [* p:n] -> honli\n",
    "    transcription = re.sub(r\"(?:^|\\s)([a-zA-Z'_-]+)@u\\s\\[\\:\\s([a-zA-Z'\\s]*)\\]\\s\\[\\*\\s*([*:a-zA-Z\\d\\s'+-=]*)\\]\", r' \\2', transcription)\n",
    "         \n",
    "    # kotəgəl@u [: comfortable] [* n:k] -> UNK\n",
    "    # hɑspəlɪd@u [: hospital] [* n:k-ret] -> UNK\n",
    "    # ðæɾɪ@u [: x@n] [* n:uk] -> UNK\n",
    "    # mɔ@u [: x@n] [* n:uk-rep] -> UNK\n",
    "    # fɪŋks@u [: sphinx] [*] -> UNK\n",
    "    transcription = re.sub(r\"(\\S*@u)\\s\\[\\:\\s([a-zA-Z'_@\\s]*)\\]\\s\\[\\*\\s*([*:a-zA-Z\\d\\s'+-=]*)\\]\", r'UNK', transcription)\n",
    "\n",
    "    # ʌt [: up] [* p:n] -> UNK\n",
    "    # iʔi [: x@n] [* n:uk] -> UNK\n",
    "    transcription = re.sub(r\"([a-zA-Z'_]*[^a-zA-Z'_\\s]+[a-zA-Z'_]*[^a-zA-Z'_\\s]*[a-zA-Z'_]*[^a-zA-Z'_\\s]*[a-zA-Z'_]*[^a-zA-Z'_\\s]*[a-zA-Z'_]*[^a-zA-Z'_\\s]*[a-zA-Z'_]*[^a-zA-Z'_\\s]*[a-zA-Z'_]*[^a-zA-Z'_\\s]*)\\s\\[\\:\\s([a-zA-Z'_@\\s]+)\\]\\s\\[\\*\\s*([*:a-zA-Z\\s'+-=]*)\\]\", r'UNK', transcription)\n",
    "    transcription = re.sub(r\"\\S*\\s\\[\\:\\s([a-zA-Z'_@$\\s]+)\\]\\s\\[\\*\\s*([*:a-zA-Z\\s'+-=]*)\\]\", r'UNK', transcription)\n",
    "                 \n",
    "    # sɪnrɛlə@u [: Cinderella] (without error code) -> UNK\n",
    "    # transcription = re.sub(r\"(\\S*@u)\\s\\[\\:\\s([a-zA-Z'_@\\s]*)\\](?!\\s\\[\\*)\", r'UNK', transcription)\n",
    "    transcription = re.sub(r\"([a-zA-Z'_]*[^a-zA-Z'_\\s]+[a-zA-Z'_]*[^a-zA-Z'_\\s]*[a-zA-Z'_]*[^a-zA-Z'_\\s]*[a-zA-Z'_]*[^a-zA-Z'_\\s]*[a-zA-Z'_]*[^a-zA-Z'_\\s]*[a-zA-Z'_]*[^a-zA-Z'_\\s]*[a-zA-Z'_]*[^a-zA-Z'_\\s]*)\\s\\[\\:\\s([a-zA-Z'_@\\s]+)\\]\", r'UNK', transcription)\n",
    "    \n",
    "    # walked [: s:uk-ret] (without replacement, error code typo) -> walked\n",
    "    transcription = re.sub(r\"(?:^|\\s)([a-zA-Z'_-]+)\\s\\[\\:\\s([a-zA-Z':\\s-]+)\\](?!\\s\\[\\*)\", r' \\1', transcription)\n",
    "    \n",
    "    # remaining error codes\n",
    "    transcription = re.sub(r'\\[\\*\\s[^\\]]+\\]', '', transcription)\n",
    "    transcription = transcription.replace('[*]', '')\n",
    "    \n",
    "    # untackled cases\n",
    "    transcription = re.sub(r\"(\\S*@u)\\s\\[\\:\\s(\\S*)\\]\", 'UNK', transcription)\n",
    "    transcription = re.sub(r\"([a-zA-Z'_]*(?!@)[^a-zA-Z'_\\s]+[a-zA-Z'_]*)\\s\\[\\:\\s([a-zA-Z'_@\\s]+)\\]\", 'UNK', transcription)\n",
    "    \n",
    "    # start of the string\n",
    "    # transcription = re.sub(r\"([a-zA-Z']+)\\s\\[:\\s([a-zA-Z']+)\\]\", r'\\1', transcription)\n",
    "                           \n",
    "    # Error codes\n",
    "    transcription = re.sub(r'\\$\\S+', '', transcription)  \n",
    "    # transcription = re.sub(r'@l', '', transcription)\n",
    "    # change letter to upper class\n",
    "    transcription = re.sub(r'\\b([a-zA-Z])@l\\b', lambda m: m.group(1).upper(), transcription)\n",
    "    transcription = re.sub(r'@o', '', transcription)\n",
    "    transcription = re.sub(r'@b', '', transcription)\n",
    "    transcription = re.sub(r'@q', '', transcription)\n",
    "    transcription = re.sub(r'@k', '', transcription)\n",
    "    transcription = re.sub(r'@i', '', transcription)\n",
    "    # transcription = re.sub(r'@n', '', transcription) \n",
    "    transcription = re.sub(r'@si', '', transcription)\n",
    "    \n",
    "    transcription = re.sub(r'([a-zA-Z]+)@n', r'\\1', transcription)\n",
    "    transcription = re.sub(r'\\S+@u', r'UNK', transcription)\n",
    "\n",
    "    # INV\n",
    "    transcription = re.sub(r'&\\*INV\\S+', '', transcription)\n",
    "    \n",
    "    # Remove words with &-, &+, &=, [=! ], [= ], [!], [% ]\n",
    "    # need to include & because typo\n",
    "    #transcription = re.sub(r'&[+-]?.*?\\s', '', transcription)\n",
    "    transcription = re.sub(r'&\\S*', '', transcription)\n",
    "    transcription = re.sub(r'&[^ ]*', '', transcription)\n",
    "    # transcription = re.sub(r'\\[=!\\s[a-zA-Z]*\\]', '', transcription)\n",
    "    transcription = transcription.replace('[!]', '')\n",
    "    transcription = re.sub(r'\\[=[^\\]]+\\]', '', transcription)\n",
    "    transcription = transcription.replace('[=! laughin', '')\n",
    "    transcription = re.sub(r\"\\[%\\s(.*)\\]\", '', transcription)\n",
    "    \n",
    "    # Remove unnecessary chars\n",
    "    transcription = transcription.replace('+', '').replace('\"', '').replace('...', '').replace('//', '').replace('/', '').replace('^', '').replace('„', '')\n",
    "    \n",
    "    # Replace _ with space\n",
    "    transcription = transcription.replace('_', ' ')\n",
    "\n",
    "    # Remove punctuation\n",
    "    transcription = transcription.replace('.', ' ')\n",
    "    transcription = transcription.replace('?', ' ')\n",
    "    transcription = transcription.replace('!', ' ')\n",
    "    transcription = transcription.replace('”', '').replace('“', '')  \n",
    "     \n",
    "    # Remove :\n",
    "    transcription = transcription.replace(':', '')\n",
    "    \n",
    "    # Replace - with whitespace\n",
    "    transcription = transcription.replace('-', ' ')\n",
    "    \n",
    "    # Remove words containing '0' - there's error code with '0\n",
    "    transcription = ' '.join(word for word in transcription.split() if '0' not in word)\n",
    "    \n",
    "    # Standardize whitespace\n",
    "    transcription = re.sub(r'\\s+', ' ', transcription)\n",
    "    \n",
    "    # Unknown - I suspect linebreak\n",
    "    transcription = transcription.replace('\u0015', '')\n",
    "\n",
    "    # transcription = transcription.replace('FILLER', '&-')\n",
    "    # transcription = transcription.replace('FRAGMENT', '&+')\n",
    "    \n",
    "    return transcription.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc4c489d-b3b7-4c34-8f8a-1bb827dfbc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "aphasia_df = pd.read_csv(f'../../data/aphasia_concat_{current_date}.csv')\n",
    "aphasia_df['clean'] = aphasia_df['raw_transcript'].apply(lambda x: clean_transcription(str(x)))\n",
    "# aphasia_df['clean_original'] = aphasia_df['clean']\n",
    "# aphasia_df['clean_error'] = aphasia_df['raw_transcript'].apply(lambda x: clean_transcription_error(str(x)))\n",
    "aphasia_df.to_csv(f'../../data/aphasia_concat_fix_{current_date}.csv', index=False)\n",
    "\n",
    "control_df = pd.read_csv(f'../../data/control_concat_{current_date}.csv')\n",
    "control_df['clean'] = control_df['raw_transcript'].apply(lambda x: clean_transcription(str(x)))\n",
    "# control_df['clean_original'] = control_df['clean']\n",
    "# control_df['clean_error'] = control_df['raw_transcript'].apply(lambda x: clean_transcription_error(str(x)))\n",
    "control_df.to_csv(f'../../data/control_concat_fix_{current_date}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e77c7e-9254-4899-b3d8-304b19e51cef",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3-2. Additional cleaning performed on `clean` using regex search of non-alphabetic characters `[^a-zA-Z',\\s]` and manually updating a lit ###\n",
    "\n",
    "Search done on spreadsheet based on regex search filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8cadb287-b8f1-4582-bb49-3a868f004618",
   "metadata": {},
   "outputs": [],
   "source": [
    "aphasia_df = pd.read_csv(f'../../data/Old data/aphasia_concat_fix_{current_date}.csv')\n",
    "aphasia_replacements = {\n",
    "#    \"NEURAL67-2_1792832_2031274.wav\": (\"him [ her]\", \"her\"),\n",
    "    \"NEURAL67-2_1792832_2031274.wav\": [(\"him [ her]\", \"him\")],\n",
    "    \"kurland01c_132769_370511.wav\": [(\"ɑbɑbɑbɑbɑ\", \"UNK\")],\n",
    "    \"kurland25b_681268_917950.wav\": [(\"haute@s\", \"haute\"), (\"bourgeoisie@s\", \"bourgeoisie\")],\n",
    "    \"NEURAL59-1_44865_280647.wav\": [(\"collet [ kɑlɪt]\", \"UNK\")],\n",
    "#    \"tcu03a_1073314_1307547.wav\": (\"bard[ ball]\", \"ball\"),\n",
    "    \"tcu03a_1073314_1307547.wav\": [(\"bard[ ball]\", \"bard\")],\n",
    "#    \"MSU04a_1239644_1419449.wav\": (\"he[ she]\", \"she\"),\n",
    "    \"MSU04a_1239644_1419449.wav\": [(\"he[ she]\", \"he\")],\n",
    "    \"adler17a_1651920_1812484.wav\": [( \" ]\", \"\")],\n",
    "    \"NEURAL42-1_546470_673027.wav\": [( \" exc]\", \"\")],\n",
    "    \"UNH06a_2135010_2235876.wav\": [(\"↫h h↫halve\", \"FRAGMENTh FRAGMENTh halve\")],\n",
    "    \"UNH10a_1360383_1433940.wav\": [(\"FRAGMENTfɪ\", \"UNK\")],\n",
    "    \"MSU04a_912746_984244.wav\": [(\" ,]\", \"\")],\n",
    "    \"MSU02a_232000_240450.wav\": [(\"Firstname r\", \"FirstnameR\"), (\"Firstname d\", \"FirstnameD\")],\n",
    "    \"thompson07b_128276_199076.wav\": [(\"∬\", \"\")],\n",
    "    \"UNH01a_675064_743878.wav\": [(\" ram]\", \"\")],\n",
    "    \"BU08a_279598_343252.wav\": [(\" am]\", \"\")],\n",
    "    \"BU02a_486009_548010.wav\": [(\" ]\", \"\")],\n",
    "    \"kurland12b_136053_193875.wav\": [(\"↑\", \"\")],\n",
    "    \"ACWT09a_1673873_1726242.wav\": [(\"tabeta [ volcano]\", \"volcano\")],\n",
    "    \"fridriksson02a_475028_525966.wav\": [(\" ]\", \"\")],\n",
    "    \"kurland02b_605125_655010.wav\": [(\" FRAGMENThɛ FRAGMENThɛ\", \" UNK UNK\")],\n",
    "    \"kansas16a_1022310_1067896.wav\": [(\" am]\", \"\")],\n",
    "    \"UNH06a_2240715_2266423.wav\": [(\" ↫lem lem lem↫lemonade\", \" FRAGMENTlem FRAGMENTlem FRAGMENTlem lemonade\")],\n",
    "    \"ACWT12a_1252133_1277726.wav\": [(\" FRAGMENTbæ FRAGMENTbæ FRAGMENTbeɪ FRAGMENTbeɪ\", \" UNK UNK UNK UNK\"), (\" FRAGMENTbæ FRAGMENTbæ\", \" UNK UNK\")],\n",
    "    \"UNH06a_1928325_1950276.wav\": [(\" ↫vu vu mu↫\", \" FRAGMENTvu FRAGMENTvu FRAGMENTmu \")],\n",
    "    \"kurland29e_704228_724804.wav\": [(\"beaucoup@sfra\", \"beaucoup\")],\n",
    "    \"UNH06a_1146121_1153861.wav\": [(\"↫s s↫\", \"FRAGMENTs FRAGMENTs \")],\n",
    "    \"UNH06a_1269070_1276487.wav\": [(\"↫te te↫\", \"FRAGMENTte FRAGMENTte] \")],\n",
    "    \"UNH06a_2831334_2836658.wav\": [(\"↫ru ru↫\", \"FRAGMENTru FRAGMENTru \")],\n",
    "    \"UNH06a_1532990_1536660.wav\": [(\"↫b b↫\", \"FRAGMENTb FRAGMENTb \")],\n",
    "    \"UNH05a_2911821_2915027.wav\": [(\"≠\", \"\")],\n",
    "    \"UNH17a_470998_481334.wav\": [(\"FirstNameG\", \"FirstnameG\")],\n",
    "    \"UNH17a_656203_676521.wav\": [(\"FirstNameJ\", \"FirstnameJ\"), (\"FirstNameA\", \"FirstnameA\"), (\"FirstNameL\", \"FirstnameL\")],\n",
    "    \"UNH17a_782685_791881.wav\": [(\"FirstNameN\", \"FirstnameN\")],\n",
    "    \"UNH17a_823164_834384.wav\": [(\"FirstNameB\", \"FirstnameB\")],\n",
    "    \"scale18a_200827_211307.wav\": [(\"and FILLERum Kansas the FRAGMENTs station in Kansas\", \"and FILLERum Kansas the FRAGMENTs station in Kansas do you remember the first time\")],\n",
    "    \"elman14a_2356292_2364616.wav\": [(\"dicshinery[ dictionary]\", \"dicshinery\")],\n",
    "    \"tap09a_275377_278447.wav\": [(\"yeah that FRAGMENTw FRAGMENTmo FRAGMENTme\", \"yeah that FRAGMENTw FRAGMENTmo FRAGMENTme give me a shaker\")],\n",
    "    \n",
    "}                                   \n",
    "                    \n",
    "for segment_name, replacements in aphasia_replacements.items():\n",
    "    condition = aphasia_df['segment_name'] == segment_name\n",
    "    for old, new in replacements:\n",
    "        aphasia_df.loc[condition, 'clean'] = aphasia_df.loc[condition, 'clean'].str.replace(old, new, regex=False)\n",
    "aphasia_df.to_csv(f'../../data/Old data/aphasia_concat_fix_{current_date}.csv', index=False)\n",
    "                                   \n",
    "control_df = pd.read_csv(f'../../data/Old data/control_concat_fix_{current_date}.csv')\n",
    "control_replacements = {\n",
    "    \"wright42a_467440_706986.wav\": [(\"federales@sspa\", \"federales\")],\n",
    "    \"richardson21_1668821_1908210.wav\": [(\"hoing@wp\", \"hoing\")],\n",
    "    \"richardson21_405805_643550.wav\": [(\"bibity@wp bobity@wp boo@wp\", \"bibity bobity boo\")],\n",
    "    \"NEURAL2-2_666955_902459.wav\": [(\"konoko@sjpn\", \"konoko\")],\n",
    "    \"UNH1051_1722235_1957113.wav\": [(\"libero@sita\", \"libero\")],\n",
    "    \"NEURAL6-1_1321076_1555428.wav\": [(\"verre@sfra\", \"verre\")],\n",
    "    \"wright29a_350_232930.wav\": [(\" [ ]\", \"\")],\n",
    "    \"wright69a_345680_545780.wav\": [(\"carabinieri@sita\", \"carabinieri\")],\n",
    "    \"NEURAL39-1_699293_880658.wav\": [(\"bon@sfra appetit@sfra\", \"bon appetit\")],\n",
    "    \"NEURAL2-1_890927_1058781.wav\": [(\"otokonoko@sjpn\", \"otokonoko\")],\n",
    "    \"NEURAL2-2_903204_1031394.wav\": [(\"akachon@sjpn akachon@sjpn watashi@sjpn no alachon@sjpn\", \"akachon akachon watashi no alachon\")],\n",
    "    \"capilouto80a_408260_523451.wav\": [(\"koumon@s ou@s ye@s\", \"koumon ou ye\")],\n",
    "    \"capilouto21a_268870_370494.wav\": [(\"merci@sfra obrigado@spor gracias@sspa\", \"\")],\n",
    "    \"capilouto09a_151107_223399.wav\": [(\"jambo@sswa\", \"\")],\n",
    "    \"capilouto59a_307038_378432.wav\": [(\"haben@sdeu sie@sdeu eis@sdeu\", \"\"), (\"wurfel@sdeu eis@sdeu wurfel@sdeu\", \"\"), (\"nein@sdeu\", \"\"), (\"wurfel@sdeu\", \"\")],\n",
    "    \"UNH1018_1893871_1944504.wav\": [(\"ruck@seng\", \"\")],\n",
    "    \"wright45a_167196_201841.wav\": [(\"buona@sita sera@sita\", \"\")],\n",
    "    \"UNH1018_1858187_1892376.wav\": [(\"ruck@seng\", \"\")],\n",
    "    \"wright45a_260500_289118.wav\": [(\"hors@sfra d'oeuvres@sfra\", \"\")],\n",
    "    \"UNH1037_329826_348156.wav\": [(\"ditziest@sdeu\", \"\")],\n",
    "    \"capilouto39a_220584_230404.wav\": [(\"comprende@sspa\", \"\")],\n",
    "    \"UNH1051_685561_689353.wav\": [(\"like carrying a tent around with you\", \"like carrying a tent around with you, literally it was so big\")],\n",
    "    \"UNH1034_2482928_2489098.wav\": [(\"yeah she's like I'm fighting for my own hand I remember seeing that I'm like\", \"yeah she's like I'm fighting for my own hand I remember seeing that I'm like you should definitely watch it\")],\n",
    "    \"wright18a_266347_272519.wav\": [(\"and consequently from that FILLERuh I had ototoxic drugs so I lost my hearing that way\", \"Were much larger than Cinderellas and it looked like the footman and the prince were going to leave.\")],\n",
    "    \n",
    "    \n",
    "}\n",
    "\n",
    "for segment_name, replacements in control_replacements.items():\n",
    "    condition = control_df['segment_name'] == segment_name\n",
    "    for old, new in replacements:\n",
    "        control_df.loc[condition, 'clean'] = control_df.loc[condition, 'clean'].str.replace(old, new, regex=False)\n",
    "control_df.to_csv(f'../../data/Old data/control_concat_fix_{current_date}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e7f145-ec52-45da-b7d4-a8880a55b563",
   "metadata": {},
   "source": [
    "## 3-3. Different versions of cleaning are done. Save everything to `aphasia/control_all_fix.csv`. ##\n",
    "`clean_v1` includes both fillers and phonological fragments.\n",
    "`clean_v2` includes phonological fragments, but fillers are removed.\n",
    "`clean_v3` removes both fillers and phonological fragments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54d0706a-574d-4387-be08-0e5610ac9333",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_version1(text):\n",
    "# fillers and phonological fragments are both left in\n",
    "\n",
    "    text = str(text)\n",
    "    return text.replace('FILLER', '').replace('FRAGMENT', '')\n",
    "\n",
    "def clean_version2(text):\n",
    "# fillers are removed, phonological fragments are left in\n",
    "\n",
    "    text = str(text)\n",
    "    text = ' '.join([word for word in text.split() if not word.startswith('FILLER')])\n",
    "    text = text.replace('FRAGMENT', '')\n",
    "    \n",
    "    return text\n",
    "\n",
    "def clean_version3(text):\n",
    "# fillers and phonological fragments are both removed\n",
    "\n",
    "    text = str(text)\n",
    "    text = ' '.join([word for word in text.split() if not word.startswith('FRAGMENT') and not word.startswith('FILLER')])\n",
    "\n",
    "    return text\n",
    "\n",
    "aphasia_df = pd.read_csv(f'../../data/Old data/aphasia_concat_fix_{current_date}.csv')\n",
    "control_df = pd.read_csv(f'../../data/Old data/control_concat_fix_{current_date}.csv')\n",
    "\n",
    "aphasia_df['clean_v1'] = aphasia_df['clean'].apply(clean_version1)\n",
    "aphasia_df['clean_v2'] = aphasia_df['clean'].apply(clean_version2)\n",
    "aphasia_df['clean_v3'] = aphasia_df['clean'].apply(clean_version3)\n",
    "\n",
    "control_df['clean_v1'] = control_df['clean'].apply(clean_version1)\n",
    "control_df['clean_v2'] = control_df['clean'].apply(clean_version2)\n",
    "control_df['clean_v3'] = control_df['clean'].apply(clean_version3)\n",
    "\n",
    "aphasia_df.to_csv(f'../../data/Old data/aphasia_concat_fix_{current_date}.csv')\n",
    "control_df.to_csv(f'../../data/Old data/control_concat_fix_{current_date}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d954374-b0ec-45a0-a879-72237089d95d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique aphasia interviews: 550\n",
      "Total number of unique control interviews: 347\n",
      "Total number of aphasia snippets: 23057\n",
      "Total number of control snippets: 5342\n"
     ]
    }
   ],
   "source": [
    "aphasia_df = pd.read_csv(f'../../data/aphasia_concat_fix_{current_date}.csv')\n",
    "control_df = pd.read_csv(f'../../data/control_concat_fix_{current_date}.csv')\n",
    "\n",
    "unique_aphasia_filenames = aphasia_df['filename'].nunique()\n",
    "unique_control_filenames = control_df['filename'].nunique()\n",
    "\n",
    "total_aphasia_snippets = aphasia_df.shape[0]\n",
    "total_control_snippets = control_df.shape[0]\n",
    "\n",
    "print(f\"Total number of unique aphasia interviews: {unique_aphasia_filenames}\")\n",
    "print(f\"Total number of unique control interviews: {unique_control_filenames}\")\n",
    "\n",
    "print(f\"Total number of aphasia snippets: {total_aphasia_snippets}\")\n",
    "print(f\"Total number of control snippets: {total_control_snippets}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53694f3-11c3-436d-b57b-f2ab54bc372c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
